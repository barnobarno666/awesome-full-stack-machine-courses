{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e93663",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-12T16:28:34.747648Z",
     "iopub.status.busy": "2023-08-12T16:28:34.746951Z",
     "iopub.status.idle": "2023-08-12T16:28:34.760132Z",
     "shell.execute_reply": "2023-08-12T16:28:34.759245Z"
    },
    "papermill": {
     "duration": 0.02159,
     "end_time": "2023-08-12T16:28:34.762176",
     "exception": false,
     "start_time": "2023-08-12T16:28:34.740586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8309ba70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T16:28:34.772147Z",
     "iopub.status.busy": "2023-08-12T16:28:34.771351Z",
     "iopub.status.idle": "2023-08-12T16:28:47.759417Z",
     "shell.execute_reply": "2023-08-12T16:28:47.758370Z"
    },
    "papermill": {
     "duration": 12.995915,
     "end_time": "2023-08-12T16:28:47.762311",
     "exception": false,
     "start_time": "2023-08-12T16:28:34.766396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib as plt \n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7fd0c3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T16:28:47.778206Z",
     "iopub.status.busy": "2023-08-12T16:28:47.777614Z",
     "iopub.status.idle": "2023-08-12T16:28:48.479805Z",
     "shell.execute_reply": "2023-08-12T16:28:48.478851Z"
    },
    "papermill": {
     "duration": 0.714139,
     "end_time": "2023-08-12T16:28:48.483844",
     "exception": false,
     "start_time": "2023-08-12T16:28:47.769705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure( (x_train[0]).size )\n",
    "for i in range(10):\n",
    "    ax=fig.add_subplot(3,12,i+1,xticks=[],yticks=[])\n",
    "    ax.imshow(x_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "745757a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T16:28:48.513532Z",
     "iopub.status.busy": "2023-08-12T16:28:48.512709Z",
     "iopub.status.idle": "2023-08-12T16:28:48.743401Z",
     "shell.execute_reply": "2023-08-12T16:28:48.742063Z"
    },
    "papermill": {
     "duration": 0.245485,
     "end_time": "2023-08-12T16:28:48.745641",
     "exception": false,
     "start_time": "2023-08-12T16:28:48.500156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\n",
      "[9]\n",
      "[4]\n",
      "[1]\n",
      "[2]\n",
      "[7]\n",
      "[8]\n",
      "[3]\n",
      "[5]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "f=[]\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] not in f:\n",
    "        print(y_train[i])\n",
    "        f.append(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19ef490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T16:28:48.762211Z",
     "iopub.status.busy": "2023-08-12T16:28:48.761922Z",
     "iopub.status.idle": "2023-08-12T16:28:49.018001Z",
     "shell.execute_reply": "2023-08-12T16:28:49.017020Z"
    },
    "papermill": {
     "duration": 0.267486,
     "end_time": "2023-08-12T16:28:49.020744",
     "exception": false,
     "start_time": "2023-08-12T16:28:48.753258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical as to_Catagorical\n",
    "y_train=to_Catagorical(y_train,num_classes=len(f))\n",
    "y_test=to_Catagorical(y_test,num_classes=len(f))\n",
    "\n",
    "x_train=x_train.astype('float32')/255\n",
    "x_test=x_test.astype('float32')/255\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "451ac32a",
   "metadata": {
    "papermill": {
     "duration": 0.007303,
     "end_time": "2023-08-12T16:28:49.036470",
     "exception": false,
     "start_time": "2023-08-12T16:28:49.029167",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b43e351",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T16:28:49.053859Z",
     "iopub.status.busy": "2023-08-12T16:28:49.052341Z",
     "iopub.status.idle": "2023-08-12T16:28:49.989148Z",
     "shell.execute_reply": "2023-08-12T16:28:49.987897Z"
    },
    "papermill": {
     "duration": 0.947756,
     "end_time": "2023-08-12T16:28:49.991669",
     "exception": false,
     "start_time": "2023-08-12T16:28:49.043913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=666)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dbc529",
   "metadata": {
    "papermill": {
     "duration": 0.00736,
     "end_time": "2023-08-12T16:28:50.006928",
     "exception": false,
     "start_time": "2023-08-12T16:28:49.999568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "the model is unsing the alexnet artitechture somewhaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f911d08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T16:28:50.024428Z",
     "iopub.status.busy": "2023-08-12T16:28:50.022887Z",
     "iopub.status.idle": "2023-08-12T16:28:54.505153Z",
     "shell.execute_reply": "2023-08-12T16:28:54.504040Z"
    },
    "papermill": {
     "duration": 4.506154,
     "end_time": "2023-08-12T16:28:54.520455",
     "exception": false,
     "start_time": "2023-08-12T16:28:50.014301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 16)        208       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 32)        2080      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 64)          8256      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               512500    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 528,054\n",
      "Trainable params: 528,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow import keras \n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "model=Sequential( [\n",
    "    \n",
    "    Conv2D(filters=16,kernel_size=2,padding='same',activation='relu',\n",
    "           input_shape=(32,32,3))\n",
    "    ,MaxPooling2D(pool_size=2)\n",
    "    ,Conv2D(filters=32,kernel_size=2,padding='same',activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(filters=64,kernel_size=2,padding='same',activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(500,activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(units=10,activation='softmax')    \n",
    "    \n",
    "    \n",
    "])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe134821",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T16:28:54.542806Z",
     "iopub.status.busy": "2023-08-12T16:28:54.542514Z",
     "iopub.status.idle": "2023-08-12T16:28:54.559561Z",
     "shell.execute_reply": "2023-08-12T16:28:54.558638Z"
    },
    "papermill": {
     "duration": 0.030524,
     "end_time": "2023-08-12T16:28:54.561683",
     "exception": false,
     "start_time": "2023-08-12T16:28:54.531159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2184ead",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-08-12T16:28:54.584074Z",
     "iopub.status.busy": "2023-08-12T16:28:54.583244Z",
     "iopub.status.idle": "2023-08-12T16:39:56.915081Z",
     "shell.execute_reply": "2023-08-12T16:39:56.914058Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 662.345812,
     "end_time": "2023-08-12T16:39:56.917720",
     "exception": false,
     "start_time": "2023-08-12T16:28:54.571908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 16:28:57.040790: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.6343 - accuracy: 0.4022\n",
      "Epoch 1: val_loss improved from inf to 1.31340, saving model to cifer_10 _weights.hdf5\n",
      "1250/1250 [==============================] - 17s 5ms/step - loss: 1.6330 - accuracy: 0.4029 - val_loss: 1.3134 - val_accuracy: 0.5374\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.3167 - accuracy: 0.5268\n",
      "Epoch 2: val_loss improved from 1.31340 to 1.13749, saving model to cifer_10 _weights.hdf5\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3167 - accuracy: 0.5268 - val_loss: 1.1375 - val_accuracy: 0.5966\n",
      "Epoch 3/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.1840 - accuracy: 0.5763\n",
      "Epoch 3: val_loss improved from 1.13749 to 1.05110, saving model to cifer_10 _weights.hdf5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.1838 - accuracy: 0.5764 - val_loss: 1.0511 - val_accuracy: 0.6252\n",
      "Epoch 4/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.0999 - accuracy: 0.6105\n",
      "Epoch 4: val_loss did not improve from 1.05110\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.1008 - accuracy: 0.6103 - val_loss: 1.0914 - val_accuracy: 0.6236\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.0372 - accuracy: 0.6367\n",
      "Epoch 5: val_loss improved from 1.05110 to 0.94787, saving model to cifer_10 _weights.hdf5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0372 - accuracy: 0.6367 - val_loss: 0.9479 - val_accuracy: 0.6652\n",
      "Epoch 6/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 0.9954 - accuracy: 0.6526\n",
      "Epoch 6: val_loss did not improve from 0.94787\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9951 - accuracy: 0.6526 - val_loss: 0.9905 - val_accuracy: 0.6604\n",
      "Epoch 7/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.9697 - accuracy: 0.6602\n",
      "Epoch 7: val_loss improved from 0.94787 to 0.93295, saving model to cifer_10 _weights.hdf5\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9693 - accuracy: 0.6603 - val_loss: 0.9329 - val_accuracy: 0.6714\n",
      "Epoch 8/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.9460 - accuracy: 0.6730\n",
      "Epoch 8: val_loss did not improve from 0.93295\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9462 - accuracy: 0.6729 - val_loss: 0.9347 - val_accuracy: 0.6786\n",
      "Epoch 9/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 0.9321 - accuracy: 0.6822\n",
      "Epoch 9: val_loss improved from 0.93295 to 0.88781, saving model to cifer_10 _weights.hdf5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9322 - accuracy: 0.6821 - val_loss: 0.8878 - val_accuracy: 0.6967\n",
      "Epoch 10/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.9145 - accuracy: 0.6843\n",
      "Epoch 10: val_loss did not improve from 0.88781\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9146 - accuracy: 0.6842 - val_loss: 0.9041 - val_accuracy: 0.6907\n",
      "Epoch 11/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.9099 - accuracy: 0.6909\n",
      "Epoch 11: val_loss did not improve from 0.88781\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9105 - accuracy: 0.6906 - val_loss: 1.0311 - val_accuracy: 0.6618\n",
      "Epoch 12/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.9109 - accuracy: 0.6913\n",
      "Epoch 12: val_loss did not improve from 0.88781\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9104 - accuracy: 0.6914 - val_loss: 0.9062 - val_accuracy: 0.6976\n",
      "Epoch 13/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 0.9056 - accuracy: 0.6917\n",
      "Epoch 13: val_loss did not improve from 0.88781\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9058 - accuracy: 0.6916 - val_loss: 0.9172 - val_accuracy: 0.6960\n",
      "Epoch 14/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.9016 - accuracy: 0.6972\n",
      "Epoch 14: val_loss did not improve from 0.88781\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9016 - accuracy: 0.6971 - val_loss: 0.8883 - val_accuracy: 0.6906\n",
      "Epoch 15/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.8982 - accuracy: 0.6999\n",
      "Epoch 15: val_loss did not improve from 0.88781\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8986 - accuracy: 0.6998 - val_loss: 0.9264 - val_accuracy: 0.6863\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.9018 - accuracy: 0.6968\n",
      "Epoch 16: val_loss did not improve from 0.88781\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9018 - accuracy: 0.6968 - val_loss: 0.9252 - val_accuracy: 0.6902\n",
      "Epoch 17/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 0.9002 - accuracy: 0.7004\n",
      "Epoch 17: val_loss did not improve from 0.88781\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9007 - accuracy: 0.7006 - val_loss: 0.8926 - val_accuracy: 0.7030\n",
      "Epoch 18/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.8993 - accuracy: 0.6973\n",
      "Epoch 18: val_loss did not improve from 0.88781\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8998 - accuracy: 0.6971 - val_loss: 0.9055 - val_accuracy: 0.7042\n",
      "Epoch 19/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 0.9163 - accuracy: 0.6980\n",
      "Epoch 19: val_loss did not improve from 0.88781\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9167 - accuracy: 0.6981 - val_loss: 0.9175 - val_accuracy: 0.6967\n",
      "Epoch 20/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.9167 - accuracy: 0.6993\n",
      "Epoch 20: val_loss did not improve from 0.88781\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9171 - accuracy: 0.6990 - val_loss: 0.9653 - val_accuracy: 0.6829\n",
      "Epoch 21/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.9212 - accuracy: 0.6958\n",
      "Epoch 21: val_loss improved from 0.88781 to 0.88451, saving model to cifer_10 _weights.hdf5\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9210 - accuracy: 0.6959 - val_loss: 0.8845 - val_accuracy: 0.6980\n",
      "Epoch 22/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 0.9269 - accuracy: 0.6950\n",
      "Epoch 22: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9279 - accuracy: 0.6949 - val_loss: 0.9182 - val_accuracy: 0.6996\n",
      "Epoch 23/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.9398 - accuracy: 0.6935\n",
      "Epoch 23: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9403 - accuracy: 0.6934 - val_loss: 1.0981 - val_accuracy: 0.6447\n",
      "Epoch 24/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.9416 - accuracy: 0.6948\n",
      "Epoch 24: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9411 - accuracy: 0.6949 - val_loss: 0.9559 - val_accuracy: 0.6849\n",
      "Epoch 25/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.9592 - accuracy: 0.6884\n",
      "Epoch 25: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9588 - accuracy: 0.6886 - val_loss: 0.9142 - val_accuracy: 0.7011\n",
      "Epoch 26/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 0.9681 - accuracy: 0.6873\n",
      "Epoch 26: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9687 - accuracy: 0.6873 - val_loss: 1.0365 - val_accuracy: 0.6750\n",
      "Epoch 27/100\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 0.9860 - accuracy: 0.6769\n",
      "Epoch 27: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9856 - accuracy: 0.6773 - val_loss: 1.0101 - val_accuracy: 0.6802\n",
      "Epoch 28/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 0.9973 - accuracy: 0.6781\n",
      "Epoch 28: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9979 - accuracy: 0.6780 - val_loss: 1.0158 - val_accuracy: 0.6801\n",
      "Epoch 29/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.0111 - accuracy: 0.6722\n",
      "Epoch 29: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0115 - accuracy: 0.6721 - val_loss: 0.9603 - val_accuracy: 0.6839\n",
      "Epoch 30/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 1.0105 - accuracy: 0.6750\n",
      "Epoch 30: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.0100 - accuracy: 0.6751 - val_loss: 1.0036 - val_accuracy: 0.6847\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.0417 - accuracy: 0.6655\n",
      "Epoch 31: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.0417 - accuracy: 0.6655 - val_loss: 1.0618 - val_accuracy: 0.6630\n",
      "Epoch 32/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 1.0543 - accuracy: 0.6586\n",
      "Epoch 32: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0545 - accuracy: 0.6586 - val_loss: 1.0413 - val_accuracy: 0.6505\n",
      "Epoch 33/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.0734 - accuracy: 0.6566\n",
      "Epoch 33: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.0736 - accuracy: 0.6567 - val_loss: 1.1236 - val_accuracy: 0.6207\n",
      "Epoch 34/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 1.0959 - accuracy: 0.6473\n",
      "Epoch 34: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0962 - accuracy: 0.6473 - val_loss: 1.0220 - val_accuracy: 0.6628\n",
      "Epoch 35/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.1083 - accuracy: 0.6466\n",
      "Epoch 35: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1089 - accuracy: 0.6464 - val_loss: 1.0786 - val_accuracy: 0.6348\n",
      "Epoch 36/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 1.1301 - accuracy: 0.6392\n",
      "Epoch 36: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1308 - accuracy: 0.6388 - val_loss: 1.0148 - val_accuracy: 0.6759\n",
      "Epoch 37/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.1437 - accuracy: 0.6322\n",
      "Epoch 37: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1437 - accuracy: 0.6321 - val_loss: 1.0691 - val_accuracy: 0.6559\n",
      "Epoch 38/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.1603 - accuracy: 0.6302\n",
      "Epoch 38: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1603 - accuracy: 0.6302 - val_loss: 1.0733 - val_accuracy: 0.6404\n",
      "Epoch 39/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.1766 - accuracy: 0.6261\n",
      "Epoch 39: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.1763 - accuracy: 0.6261 - val_loss: 1.0557 - val_accuracy: 0.6564\n",
      "Epoch 40/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 1.1919 - accuracy: 0.6180\n",
      "Epoch 40: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.1920 - accuracy: 0.6179 - val_loss: 1.2375 - val_accuracy: 0.6046\n",
      "Epoch 41/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 1.2114 - accuracy: 0.6120\n",
      "Epoch 41: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.2116 - accuracy: 0.6119 - val_loss: 1.1313 - val_accuracy: 0.6277\n",
      "Epoch 42/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.2176 - accuracy: 0.6077\n",
      "Epoch 42: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2184 - accuracy: 0.6075 - val_loss: 1.1550 - val_accuracy: 0.6192\n",
      "Epoch 43/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.2390 - accuracy: 0.6022\n",
      "Epoch 43: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.2373 - accuracy: 0.6024 - val_loss: 1.1636 - val_accuracy: 0.6301\n",
      "Epoch 44/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.2455 - accuracy: 0.6023\n",
      "Epoch 44: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2460 - accuracy: 0.6022 - val_loss: 1.0423 - val_accuracy: 0.6491\n",
      "Epoch 45/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.2585 - accuracy: 0.5948\n",
      "Epoch 45: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.2585 - accuracy: 0.5948 - val_loss: 1.0934 - val_accuracy: 0.6497\n",
      "Epoch 46/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.2774 - accuracy: 0.5939\n",
      "Epoch 46: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.2776 - accuracy: 0.5936 - val_loss: 1.0741 - val_accuracy: 0.6414\n",
      "Epoch 47/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.2732 - accuracy: 0.5907\n",
      "Epoch 47: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.2732 - accuracy: 0.5907 - val_loss: 1.2532 - val_accuracy: 0.6070\n",
      "Epoch 48/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 1.2880 - accuracy: 0.5836\n",
      "Epoch 48: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2874 - accuracy: 0.5839 - val_loss: 1.1164 - val_accuracy: 0.6369\n",
      "Epoch 49/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 1.2968 - accuracy: 0.5869\n",
      "Epoch 49: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.2971 - accuracy: 0.5868 - val_loss: 1.0972 - val_accuracy: 0.6399\n",
      "Epoch 50/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.2960 - accuracy: 0.5794\n",
      "Epoch 50: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2973 - accuracy: 0.5794 - val_loss: 1.2226 - val_accuracy: 0.6109\n",
      "Epoch 51/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.3238 - accuracy: 0.5761\n",
      "Epoch 51: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3242 - accuracy: 0.5759 - val_loss: 1.3331 - val_accuracy: 0.5732\n",
      "Epoch 52/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.3191 - accuracy: 0.5758\n",
      "Epoch 52: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3192 - accuracy: 0.5756 - val_loss: 1.1290 - val_accuracy: 0.6248\n",
      "Epoch 53/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.3529 - accuracy: 0.5691\n",
      "Epoch 53: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3524 - accuracy: 0.5694 - val_loss: 1.1393 - val_accuracy: 0.6323\n",
      "Epoch 54/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.3619 - accuracy: 0.5648\n",
      "Epoch 54: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3619 - accuracy: 0.5646 - val_loss: 1.1230 - val_accuracy: 0.6294\n",
      "Epoch 55/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.3698 - accuracy: 0.5613\n",
      "Epoch 55: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3695 - accuracy: 0.5613 - val_loss: 1.0851 - val_accuracy: 0.6449\n",
      "Epoch 56/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 1.3669 - accuracy: 0.5595\n",
      "Epoch 56: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3677 - accuracy: 0.5597 - val_loss: 1.3507 - val_accuracy: 0.5283\n",
      "Epoch 57/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.3690 - accuracy: 0.5616\n",
      "Epoch 57: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3690 - accuracy: 0.5616 - val_loss: 1.1043 - val_accuracy: 0.6336\n",
      "Epoch 58/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 1.3691 - accuracy: 0.5568\n",
      "Epoch 58: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3686 - accuracy: 0.5569 - val_loss: 1.4329 - val_accuracy: 0.5571\n",
      "Epoch 59/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 1.3695 - accuracy: 0.5589\n",
      "Epoch 59: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3685 - accuracy: 0.5592 - val_loss: 1.1553 - val_accuracy: 0.6136\n",
      "Epoch 60/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 1.3944 - accuracy: 0.5499\n",
      "Epoch 60: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3953 - accuracy: 0.5495 - val_loss: 1.1538 - val_accuracy: 0.6141\n",
      "Epoch 61/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.3827 - accuracy: 0.5513\n",
      "Epoch 61: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3827 - accuracy: 0.5513 - val_loss: 1.1715 - val_accuracy: 0.6029\n",
      "Epoch 62/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.4023 - accuracy: 0.5508\n",
      "Epoch 62: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4032 - accuracy: 0.5502 - val_loss: 1.2060 - val_accuracy: 0.5867\n",
      "Epoch 63/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.3999 - accuracy: 0.5498\n",
      "Epoch 63: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3989 - accuracy: 0.5500 - val_loss: 1.3212 - val_accuracy: 0.5541\n",
      "Epoch 64/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.4018 - accuracy: 0.5479\n",
      "Epoch 64: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4025 - accuracy: 0.5475 - val_loss: 1.3842 - val_accuracy: 0.5222\n",
      "Epoch 65/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.4007 - accuracy: 0.5475\n",
      "Epoch 65: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4004 - accuracy: 0.5474 - val_loss: 1.2459 - val_accuracy: 0.5687\n",
      "Epoch 66/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.4162 - accuracy: 0.5437\n",
      "Epoch 66: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.4164 - accuracy: 0.5436 - val_loss: 1.4218 - val_accuracy: 0.4889\n",
      "Epoch 67/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 1.4185 - accuracy: 0.5403\n",
      "Epoch 67: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4188 - accuracy: 0.5402 - val_loss: 1.4159 - val_accuracy: 0.5020\n",
      "Epoch 68/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 1.4368 - accuracy: 0.5357\n",
      "Epoch 68: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4372 - accuracy: 0.5354 - val_loss: 1.2975 - val_accuracy: 0.5746\n",
      "Epoch 69/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.4498 - accuracy: 0.5317\n",
      "Epoch 69: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4516 - accuracy: 0.5315 - val_loss: 1.5776 - val_accuracy: 0.4489\n",
      "Epoch 70/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.4430 - accuracy: 0.5321\n",
      "Epoch 70: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4436 - accuracy: 0.5319 - val_loss: 1.4089 - val_accuracy: 0.5236\n",
      "Epoch 71/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.4600 - accuracy: 0.5265\n",
      "Epoch 71: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4609 - accuracy: 0.5260 - val_loss: 1.3850 - val_accuracy: 0.5247\n",
      "Epoch 72/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.4602 - accuracy: 0.5272\n",
      "Epoch 72: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4602 - accuracy: 0.5272 - val_loss: 1.2203 - val_accuracy: 0.5886\n",
      "Epoch 73/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 1.4567 - accuracy: 0.5270\n",
      "Epoch 73: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4565 - accuracy: 0.5271 - val_loss: 1.2147 - val_accuracy: 0.5693\n",
      "Epoch 74/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.4641 - accuracy: 0.5246\n",
      "Epoch 74: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4641 - accuracy: 0.5246 - val_loss: 1.3844 - val_accuracy: 0.5459\n",
      "Epoch 75/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.4834 - accuracy: 0.5190\n",
      "Epoch 75: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.4834 - accuracy: 0.5187 - val_loss: 1.4342 - val_accuracy: 0.4906\n",
      "Epoch 76/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.4931 - accuracy: 0.5130\n",
      "Epoch 76: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4931 - accuracy: 0.5130 - val_loss: 1.3076 - val_accuracy: 0.5447\n",
      "Epoch 77/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.5212 - accuracy: 0.5041\n",
      "Epoch 77: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5215 - accuracy: 0.5045 - val_loss: 1.3128 - val_accuracy: 0.5510\n",
      "Epoch 78/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 1.4992 - accuracy: 0.5104\n",
      "Epoch 78: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4990 - accuracy: 0.5104 - val_loss: 1.2956 - val_accuracy: 0.5633\n",
      "Epoch 79/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.4992 - accuracy: 0.5092\n",
      "Epoch 79: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4992 - accuracy: 0.5092 - val_loss: 1.3290 - val_accuracy: 0.5365\n",
      "Epoch 80/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.4991 - accuracy: 0.5069\n",
      "Epoch 80: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.5000 - accuracy: 0.5069 - val_loss: 1.6894 - val_accuracy: 0.3995\n",
      "Epoch 81/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.5070 - accuracy: 0.5053\n",
      "Epoch 81: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5070 - accuracy: 0.5053 - val_loss: 1.2057 - val_accuracy: 0.5843\n",
      "Epoch 82/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.5100 - accuracy: 0.5081\n",
      "Epoch 82: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5099 - accuracy: 0.5080 - val_loss: 1.2687 - val_accuracy: 0.5880\n",
      "Epoch 83/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.5090 - accuracy: 0.5040\n",
      "Epoch 83: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5086 - accuracy: 0.5042 - val_loss: 1.4571 - val_accuracy: 0.4835\n",
      "Epoch 84/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.5074 - accuracy: 0.5040\n",
      "Epoch 84: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.5074 - accuracy: 0.5040 - val_loss: 1.4419 - val_accuracy: 0.4777\n",
      "Epoch 85/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.5173 - accuracy: 0.5043\n",
      "Epoch 85: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.5179 - accuracy: 0.5043 - val_loss: 1.2191 - val_accuracy: 0.5828\n",
      "Epoch 86/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.5020 - accuracy: 0.5066\n",
      "Epoch 86: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.5024 - accuracy: 0.5068 - val_loss: 1.4357 - val_accuracy: 0.5387\n",
      "Epoch 87/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.5121 - accuracy: 0.5056\n",
      "Epoch 87: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5117 - accuracy: 0.5058 - val_loss: 1.4392 - val_accuracy: 0.5324\n",
      "Epoch 88/100\n",
      "1238/1250 [============================>.] - ETA: 0s - loss: 1.4968 - accuracy: 0.5097\n",
      "Epoch 88: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4981 - accuracy: 0.5090 - val_loss: 1.2855 - val_accuracy: 0.5630\n",
      "Epoch 89/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 1.4957 - accuracy: 0.5099\n",
      "Epoch 89: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4951 - accuracy: 0.5100 - val_loss: 1.3581 - val_accuracy: 0.5998\n",
      "Epoch 90/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.5045 - accuracy: 0.5078\n",
      "Epoch 90: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.5044 - accuracy: 0.5078 - val_loss: 1.3145 - val_accuracy: 0.5658\n",
      "Epoch 91/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.4937 - accuracy: 0.5113\n",
      "Epoch 91: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4937 - accuracy: 0.5113 - val_loss: 1.4673 - val_accuracy: 0.4978\n",
      "Epoch 92/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.5155 - accuracy: 0.5021\n",
      "Epoch 92: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.5176 - accuracy: 0.5020 - val_loss: 1.7896 - val_accuracy: 0.3814\n",
      "Epoch 93/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.5079 - accuracy: 0.5053\n",
      "Epoch 93: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.5082 - accuracy: 0.5051 - val_loss: 1.4589 - val_accuracy: 0.5309\n",
      "Epoch 94/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.5157 - accuracy: 0.5028\n",
      "Epoch 94: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.5148 - accuracy: 0.5031 - val_loss: 1.1871 - val_accuracy: 0.5876\n",
      "Epoch 95/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 1.4921 - accuracy: 0.5090\n",
      "Epoch 95: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.4920 - accuracy: 0.5089 - val_loss: 1.2191 - val_accuracy: 0.5734\n",
      "Epoch 96/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 1.4972 - accuracy: 0.5061\n",
      "Epoch 96: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4987 - accuracy: 0.5061 - val_loss: 1.3595 - val_accuracy: 0.5054\n",
      "Epoch 97/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 1.5195 - accuracy: 0.5064\n",
      "Epoch 97: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.5192 - accuracy: 0.5065 - val_loss: 1.3704 - val_accuracy: 0.5377\n",
      "Epoch 98/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 1.4929 - accuracy: 0.5078\n",
      "Epoch 98: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4931 - accuracy: 0.5078 - val_loss: 1.3911 - val_accuracy: 0.5430\n",
      "Epoch 99/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.5030 - accuracy: 0.5056\n",
      "Epoch 99: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.5017 - accuracy: 0.5060 - val_loss: 1.6078 - val_accuracy: 0.5417\n",
      "Epoch 100/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 1.5088 - accuracy: 0.5049\n",
      "Epoch 100: val_loss did not improve from 0.88451\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.5090 - accuracy: 0.5048 - val_loss: 1.3404 - val_accuracy: 0.5568\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='cifer_10 _weights.hdf5', verbose=1, save_best_only=True)\n",
    "hist=model.fit(X_train,Y_train,epochs=100,batch_size=32,validation_data=(x_test,y_test),callbacks=[checkpointer],verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd81ef",
   "metadata": {
    "papermill": {
     "duration": 0.924953,
     "end_time": "2023-08-12T16:39:58.704574",
     "exception": false,
     "start_time": "2023-08-12T16:39:57.779621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TRAINING WITH DIFFERENT DATA EACH TIME ,so it doesnt work becasue epochs us ===is not a while loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fd2f34f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T16:40:00.557042Z",
     "iopub.status.busy": "2023-08-12T16:40:00.556483Z",
     "iopub.status.idle": "2023-08-12T16:50:53.040873Z",
     "shell.execute_reply": "2023-08-12T16:50:53.039877Z"
    },
    "papermill": {
     "duration": 655.235435,
     "end_time": "2023-08-12T16:50:54.808296",
     "exception": false,
     "start_time": "2023-08-12T16:39:59.572861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 16:40:02.878466: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_1/dropout_2/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248/1250 [============================>.] - ETA: 0s - loss: 1.6307 - accuracy: 0.4100\n",
      "Epoch 1: val_loss improved from inf to 1.30224, saving model to cifer_10 _weights_RANDOM.hdf5\n",
      "1250/1250 [==============================] - 9s 6ms/step - loss: 1.6303 - accuracy: 0.4101 - val_loss: 1.3022 - val_accuracy: 0.5312\n",
      "Epoch 2/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.2994 - accuracy: 0.5358\n",
      "Epoch 2: val_loss improved from 1.30224 to 1.13771, saving model to cifer_10 _weights_RANDOM.hdf5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.2988 - accuracy: 0.5359 - val_loss: 1.1377 - val_accuracy: 0.5949\n",
      "Epoch 3/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 1.1663 - accuracy: 0.5859\n",
      "Epoch 3: val_loss improved from 1.13771 to 1.05115, saving model to cifer_10 _weights_RANDOM.hdf5\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1660 - accuracy: 0.5858 - val_loss: 1.0512 - val_accuracy: 0.6285\n",
      "Epoch 4/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.0841 - accuracy: 0.6175\n",
      "Epoch 4: val_loss improved from 1.05115 to 0.98545, saving model to cifer_10 _weights_RANDOM.hdf5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0838 - accuracy: 0.6176 - val_loss: 0.9854 - val_accuracy: 0.6485\n",
      "Epoch 5/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.0262 - accuracy: 0.6394\n",
      "Epoch 5: val_loss did not improve from 0.98545\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0262 - accuracy: 0.6394 - val_loss: 1.0017 - val_accuracy: 0.6456\n",
      "Epoch 6/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 0.9895 - accuracy: 0.6547\n",
      "Epoch 6: val_loss improved from 0.98545 to 0.94075, saving model to cifer_10 _weights_RANDOM.hdf5\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9892 - accuracy: 0.6549 - val_loss: 0.9407 - val_accuracy: 0.6726\n",
      "Epoch 7/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 0.9649 - accuracy: 0.6669\n",
      "Epoch 7: val_loss improved from 0.94075 to 0.93536, saving model to cifer_10 _weights_RANDOM.hdf5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9647 - accuracy: 0.6668 - val_loss: 0.9354 - val_accuracy: 0.6762\n",
      "Epoch 8/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.9374 - accuracy: 0.6768\n",
      "Epoch 8: val_loss improved from 0.93536 to 0.87775, saving model to cifer_10 _weights_RANDOM.hdf5\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9371 - accuracy: 0.6769 - val_loss: 0.8777 - val_accuracy: 0.6986\n",
      "Epoch 9/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.9189 - accuracy: 0.6817\n",
      "Epoch 9: val_loss did not improve from 0.87775\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9194 - accuracy: 0.6816 - val_loss: 0.9133 - val_accuracy: 0.6882\n",
      "Epoch 10/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.9056 - accuracy: 0.6868\n",
      "Epoch 10: val_loss did not improve from 0.87775\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9053 - accuracy: 0.6869 - val_loss: 0.8809 - val_accuracy: 0.6997\n",
      "Epoch 11/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 0.8967 - accuracy: 0.6963\n",
      "Epoch 11: val_loss improved from 0.87775 to 0.86581, saving model to cifer_10 _weights_RANDOM.hdf5\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8964 - accuracy: 0.6964 - val_loss: 0.8658 - val_accuracy: 0.7076\n",
      "Epoch 12/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 0.8839 - accuracy: 0.7012\n",
      "Epoch 12: val_loss did not improve from 0.86581\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8841 - accuracy: 0.7010 - val_loss: 0.8939 - val_accuracy: 0.6984\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.8782 - accuracy: 0.7010\n",
      "Epoch 13: val_loss did not improve from 0.86581\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8782 - accuracy: 0.7010 - val_loss: 0.8686 - val_accuracy: 0.7065\n",
      "Epoch 14/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 0.8699 - accuracy: 0.7072\n",
      "Epoch 14: val_loss did not improve from 0.86581\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8698 - accuracy: 0.7073 - val_loss: 0.9123 - val_accuracy: 0.6934\n",
      "Epoch 15/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 0.8689 - accuracy: 0.7063\n",
      "Epoch 15: val_loss improved from 0.86581 to 0.85190, saving model to cifer_10 _weights_RANDOM.hdf5\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8697 - accuracy: 0.7059 - val_loss: 0.8519 - val_accuracy: 0.7083\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.8615 - accuracy: 0.7087\n",
      "Epoch 16: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8615 - accuracy: 0.7087 - val_loss: 0.9129 - val_accuracy: 0.6960\n",
      "Epoch 17/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 0.8666 - accuracy: 0.7092\n",
      "Epoch 17: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8666 - accuracy: 0.7092 - val_loss: 0.9068 - val_accuracy: 0.7006\n",
      "Epoch 18/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 0.8688 - accuracy: 0.7124\n",
      "Epoch 18: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8669 - accuracy: 0.7129 - val_loss: 0.8550 - val_accuracy: 0.7125\n",
      "Epoch 19/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 0.8729 - accuracy: 0.7100\n",
      "Epoch 19: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8724 - accuracy: 0.7101 - val_loss: 0.9114 - val_accuracy: 0.7097\n",
      "Epoch 20/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 0.8754 - accuracy: 0.7116\n",
      "Epoch 20: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8752 - accuracy: 0.7117 - val_loss: 0.9472 - val_accuracy: 0.6737\n",
      "Epoch 21/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 0.8873 - accuracy: 0.7116\n",
      "Epoch 21: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8880 - accuracy: 0.7114 - val_loss: 0.8976 - val_accuracy: 0.7066\n",
      "Epoch 22/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 0.8961 - accuracy: 0.7044\n",
      "Epoch 22: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8962 - accuracy: 0.7045 - val_loss: 0.9613 - val_accuracy: 0.6918\n",
      "Epoch 23/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.9006 - accuracy: 0.7040\n",
      "Epoch 23: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9008 - accuracy: 0.7039 - val_loss: 0.9029 - val_accuracy: 0.6974\n",
      "Epoch 24/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 0.9077 - accuracy: 0.7007\n",
      "Epoch 24: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9078 - accuracy: 0.7007 - val_loss: 0.9296 - val_accuracy: 0.6908\n",
      "Epoch 25/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 0.9190 - accuracy: 0.6980\n",
      "Epoch 25: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9191 - accuracy: 0.6980 - val_loss: 0.9697 - val_accuracy: 0.6783\n",
      "Epoch 26/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 0.9278 - accuracy: 0.6962\n",
      "Epoch 26: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9276 - accuracy: 0.6960 - val_loss: 0.9521 - val_accuracy: 0.6906\n",
      "Epoch 27/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.9362 - accuracy: 0.6953\n",
      "Epoch 27: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9365 - accuracy: 0.6951 - val_loss: 0.9378 - val_accuracy: 0.6926\n",
      "Epoch 28/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 0.9579 - accuracy: 0.6902\n",
      "Epoch 28: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9573 - accuracy: 0.6905 - val_loss: 1.1388 - val_accuracy: 0.6848\n",
      "Epoch 29/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 0.9595 - accuracy: 0.6883\n",
      "Epoch 29: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9599 - accuracy: 0.6884 - val_loss: 0.9634 - val_accuracy: 0.6828\n",
      "Epoch 30/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.9811 - accuracy: 0.6802\n",
      "Epoch 30: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9807 - accuracy: 0.6805 - val_loss: 1.0199 - val_accuracy: 0.6775\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.0064 - accuracy: 0.6758\n",
      "Epoch 31: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.0064 - accuracy: 0.6758 - val_loss: 0.9728 - val_accuracy: 0.6949\n",
      "Epoch 32/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.0129 - accuracy: 0.6742\n",
      "Epoch 32: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.0129 - accuracy: 0.6742 - val_loss: 1.2510 - val_accuracy: 0.6535\n",
      "Epoch 33/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.0196 - accuracy: 0.6712\n",
      "Epoch 33: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0196 - accuracy: 0.6711 - val_loss: 1.0363 - val_accuracy: 0.6579\n",
      "Epoch 34/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 1.0354 - accuracy: 0.6688\n",
      "Epoch 34: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0350 - accuracy: 0.6688 - val_loss: 0.9917 - val_accuracy: 0.6878\n",
      "Epoch 35/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.0562 - accuracy: 0.6623\n",
      "Epoch 35: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.0569 - accuracy: 0.6622 - val_loss: 1.0063 - val_accuracy: 0.6732\n",
      "Epoch 36/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.0557 - accuracy: 0.6578\n",
      "Epoch 36: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.0566 - accuracy: 0.6577 - val_loss: 1.1326 - val_accuracy: 0.6546\n",
      "Epoch 37/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 1.0735 - accuracy: 0.6572\n",
      "Epoch 37: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.0749 - accuracy: 0.6568 - val_loss: 1.0270 - val_accuracy: 0.6647\n",
      "Epoch 38/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.0930 - accuracy: 0.6536\n",
      "Epoch 38: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0935 - accuracy: 0.6535 - val_loss: 1.1948 - val_accuracy: 0.6336\n",
      "Epoch 39/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.1008 - accuracy: 0.6462\n",
      "Epoch 39: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.1011 - accuracy: 0.6460 - val_loss: 1.0516 - val_accuracy: 0.6610\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.1187 - accuracy: 0.6409\n",
      "Epoch 40: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1187 - accuracy: 0.6409 - val_loss: 1.0064 - val_accuracy: 0.6595\n",
      "Epoch 41/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 1.1284 - accuracy: 0.6388\n",
      "Epoch 41: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1281 - accuracy: 0.6388 - val_loss: 0.9949 - val_accuracy: 0.6897\n",
      "Epoch 42/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.1453 - accuracy: 0.6352\n",
      "Epoch 42: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.1454 - accuracy: 0.6352 - val_loss: 1.0768 - val_accuracy: 0.6417\n",
      "Epoch 43/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 1.1645 - accuracy: 0.6275\n",
      "Epoch 43: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.1645 - accuracy: 0.6274 - val_loss: 1.1099 - val_accuracy: 0.6671\n",
      "Epoch 44/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 1.1778 - accuracy: 0.6246\n",
      "Epoch 44: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1778 - accuracy: 0.6246 - val_loss: 1.1051 - val_accuracy: 0.6510\n",
      "Epoch 45/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 1.1925 - accuracy: 0.6207\n",
      "Epoch 45: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1931 - accuracy: 0.6203 - val_loss: 1.0126 - val_accuracy: 0.6643\n",
      "Epoch 46/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.1981 - accuracy: 0.6183\n",
      "Epoch 46: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1980 - accuracy: 0.6184 - val_loss: 1.1013 - val_accuracy: 0.6326\n",
      "Epoch 47/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.2055 - accuracy: 0.6130\n",
      "Epoch 47: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.2046 - accuracy: 0.6134 - val_loss: 1.1568 - val_accuracy: 0.6203\n",
      "Epoch 48/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.2156 - accuracy: 0.6130\n",
      "Epoch 48: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2156 - accuracy: 0.6131 - val_loss: 1.2717 - val_accuracy: 0.6476\n",
      "Epoch 49/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.2222 - accuracy: 0.6108\n",
      "Epoch 49: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2219 - accuracy: 0.6109 - val_loss: 1.1145 - val_accuracy: 0.6368\n",
      "Epoch 50/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 1.2237 - accuracy: 0.6050\n",
      "Epoch 50: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2237 - accuracy: 0.6053 - val_loss: 1.1038 - val_accuracy: 0.6239\n",
      "Epoch 51/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.2473 - accuracy: 0.6003\n",
      "Epoch 51: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.2475 - accuracy: 0.6003 - val_loss: 1.2514 - val_accuracy: 0.6001\n",
      "Epoch 52/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 1.2492 - accuracy: 0.6001\n",
      "Epoch 52: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.2475 - accuracy: 0.6007 - val_loss: 1.1117 - val_accuracy: 0.6104\n",
      "Epoch 53/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 1.2624 - accuracy: 0.5962\n",
      "Epoch 53: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2631 - accuracy: 0.5962 - val_loss: 1.1430 - val_accuracy: 0.6175\n",
      "Epoch 54/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.2689 - accuracy: 0.5945\n",
      "Epoch 54: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.2690 - accuracy: 0.5944 - val_loss: 1.0559 - val_accuracy: 0.6524\n",
      "Epoch 55/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.2642 - accuracy: 0.5910\n",
      "Epoch 55: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.2635 - accuracy: 0.5910 - val_loss: 1.0582 - val_accuracy: 0.6457\n",
      "Epoch 56/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.2734 - accuracy: 0.5935\n",
      "Epoch 56: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.2736 - accuracy: 0.5934 - val_loss: 1.1704 - val_accuracy: 0.5879\n",
      "Epoch 57/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.2889 - accuracy: 0.5874\n",
      "Epoch 57: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.2879 - accuracy: 0.5878 - val_loss: 1.4109 - val_accuracy: 0.5563\n",
      "Epoch 58/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 1.3080 - accuracy: 0.5866\n",
      "Epoch 58: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3073 - accuracy: 0.5865 - val_loss: 1.1369 - val_accuracy: 0.6228\n",
      "Epoch 59/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 1.3032 - accuracy: 0.5802\n",
      "Epoch 59: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3029 - accuracy: 0.5804 - val_loss: 1.2222 - val_accuracy: 0.6114\n",
      "Epoch 60/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.3177 - accuracy: 0.5787\n",
      "Epoch 60: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3186 - accuracy: 0.5788 - val_loss: 1.4095 - val_accuracy: 0.5199\n",
      "Epoch 61/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.3104 - accuracy: 0.5773\n",
      "Epoch 61: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3104 - accuracy: 0.5773 - val_loss: 1.0525 - val_accuracy: 0.6427\n",
      "Epoch 62/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.3212 - accuracy: 0.5773\n",
      "Epoch 62: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3207 - accuracy: 0.5774 - val_loss: 1.1845 - val_accuracy: 0.6287\n",
      "Epoch 63/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 1.3274 - accuracy: 0.5769\n",
      "Epoch 63: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3269 - accuracy: 0.5770 - val_loss: 1.1893 - val_accuracy: 0.6121\n",
      "Epoch 64/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.3349 - accuracy: 0.5718\n",
      "Epoch 64: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3351 - accuracy: 0.5717 - val_loss: 1.2110 - val_accuracy: 0.5769\n",
      "Epoch 65/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 1.3350 - accuracy: 0.5709\n",
      "Epoch 65: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3341 - accuracy: 0.5711 - val_loss: 1.2057 - val_accuracy: 0.6225\n",
      "Epoch 66/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 1.3377 - accuracy: 0.5707\n",
      "Epoch 66: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3380 - accuracy: 0.5704 - val_loss: 1.2516 - val_accuracy: 0.5711\n",
      "Epoch 67/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.3704 - accuracy: 0.5615\n",
      "Epoch 67: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3708 - accuracy: 0.5615 - val_loss: 1.4064 - val_accuracy: 0.5357\n",
      "Epoch 68/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 1.3502 - accuracy: 0.5646\n",
      "Epoch 68: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3496 - accuracy: 0.5647 - val_loss: 1.0712 - val_accuracy: 0.6431\n",
      "Epoch 69/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.3524 - accuracy: 0.5619\n",
      "Epoch 69: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3524 - accuracy: 0.5619 - val_loss: 1.1060 - val_accuracy: 0.6290\n",
      "Epoch 70/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 1.3594 - accuracy: 0.5633\n",
      "Epoch 70: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3589 - accuracy: 0.5636 - val_loss: 1.4242 - val_accuracy: 0.5551\n",
      "Epoch 71/100\n",
      "1248/1250 [============================>.] - ETA: 0s - loss: 1.3687 - accuracy: 0.5562\n",
      "Epoch 71: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3687 - accuracy: 0.5561 - val_loss: 1.2235 - val_accuracy: 0.5863\n",
      "Epoch 72/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.3905 - accuracy: 0.5539\n",
      "Epoch 72: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3901 - accuracy: 0.5540 - val_loss: 1.2669 - val_accuracy: 0.6176\n",
      "Epoch 73/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 1.3730 - accuracy: 0.5560\n",
      "Epoch 73: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3733 - accuracy: 0.5561 - val_loss: 1.5405 - val_accuracy: 0.4371\n",
      "Epoch 74/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.3826 - accuracy: 0.5516\n",
      "Epoch 74: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3826 - accuracy: 0.5516 - val_loss: 1.2020 - val_accuracy: 0.6027\n",
      "Epoch 75/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 1.3964 - accuracy: 0.5496\n",
      "Epoch 75: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3966 - accuracy: 0.5497 - val_loss: 1.2129 - val_accuracy: 0.6012\n",
      "Epoch 76/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.4234 - accuracy: 0.5469\n",
      "Epoch 76: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4232 - accuracy: 0.5470 - val_loss: 1.1649 - val_accuracy: 0.6140\n",
      "Epoch 77/100\n",
      "1241/1250 [============================>.] - ETA: 0s - loss: 1.4206 - accuracy: 0.5390\n",
      "Epoch 77: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4213 - accuracy: 0.5386 - val_loss: 1.2692 - val_accuracy: 0.5730\n",
      "Epoch 78/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.4248 - accuracy: 0.5373\n",
      "Epoch 78: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4252 - accuracy: 0.5372 - val_loss: 1.3479 - val_accuracy: 0.5433\n",
      "Epoch 79/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.4121 - accuracy: 0.5412\n",
      "Epoch 79: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4128 - accuracy: 0.5410 - val_loss: 1.3959 - val_accuracy: 0.5193\n",
      "Epoch 80/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.4443 - accuracy: 0.5341\n",
      "Epoch 80: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.4441 - accuracy: 0.5342 - val_loss: 1.1816 - val_accuracy: 0.6075\n",
      "Epoch 81/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.4621 - accuracy: 0.5295\n",
      "Epoch 81: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4622 - accuracy: 0.5296 - val_loss: 1.2139 - val_accuracy: 0.5832\n",
      "Epoch 82/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.4590 - accuracy: 0.5293\n",
      "Epoch 82: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4587 - accuracy: 0.5294 - val_loss: 1.2043 - val_accuracy: 0.5887\n",
      "Epoch 83/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.4652 - accuracy: 0.5243\n",
      "Epoch 83: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4649 - accuracy: 0.5244 - val_loss: 1.2585 - val_accuracy: 0.5901\n",
      "Epoch 84/100\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 1.4906 - accuracy: 0.5173\n",
      "Epoch 84: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4907 - accuracy: 0.5173 - val_loss: 1.2028 - val_accuracy: 0.5967\n",
      "Epoch 85/100\n",
      "1247/1250 [============================>.] - ETA: 0s - loss: 1.4736 - accuracy: 0.5197\n",
      "Epoch 85: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4747 - accuracy: 0.5197 - val_loss: 1.4238 - val_accuracy: 0.5115\n",
      "Epoch 86/100\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.4781 - accuracy: 0.5167\n",
      "Epoch 86: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.4781 - accuracy: 0.5167 - val_loss: 1.3528 - val_accuracy: 0.5514\n",
      "Epoch 87/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.4900 - accuracy: 0.5147\n",
      "Epoch 87: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4901 - accuracy: 0.5146 - val_loss: 1.2598 - val_accuracy: 0.6125\n",
      "Epoch 88/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.4995 - accuracy: 0.5152\n",
      "Epoch 88: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5003 - accuracy: 0.5150 - val_loss: 1.2789 - val_accuracy: 0.5611\n",
      "Epoch 89/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.5127 - accuracy: 0.5126\n",
      "Epoch 89: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5127 - accuracy: 0.5125 - val_loss: 1.3695 - val_accuracy: 0.5366\n",
      "Epoch 90/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.5010 - accuracy: 0.5095\n",
      "Epoch 90: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.5027 - accuracy: 0.5085 - val_loss: 1.2711 - val_accuracy: 0.5721\n",
      "Epoch 91/100\n",
      "1244/1250 [============================>.] - ETA: 0s - loss: 1.5147 - accuracy: 0.5038\n",
      "Epoch 91: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.5148 - accuracy: 0.5038 - val_loss: 1.2528 - val_accuracy: 0.5785\n",
      "Epoch 92/100\n",
      "1243/1250 [============================>.] - ETA: 0s - loss: 1.5192 - accuracy: 0.4982\n",
      "Epoch 92: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5189 - accuracy: 0.4983 - val_loss: 1.2789 - val_accuracy: 0.5727\n",
      "Epoch 93/100\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 1.5145 - accuracy: 0.5071\n",
      "Epoch 93: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5157 - accuracy: 0.5068 - val_loss: 1.2642 - val_accuracy: 0.5541\n",
      "Epoch 94/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.5269 - accuracy: 0.5022\n",
      "Epoch 94: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5268 - accuracy: 0.5020 - val_loss: 1.3151 - val_accuracy: 0.5540\n",
      "Epoch 95/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.5319 - accuracy: 0.4991\n",
      "Epoch 95: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5316 - accuracy: 0.4993 - val_loss: 1.1632 - val_accuracy: 0.6041\n",
      "Epoch 96/100\n",
      "1240/1250 [============================>.] - ETA: 0s - loss: 1.5229 - accuracy: 0.4989\n",
      "Epoch 96: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.5230 - accuracy: 0.4988 - val_loss: 1.4637 - val_accuracy: 0.5083\n",
      "Epoch 97/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.5264 - accuracy: 0.4980\n",
      "Epoch 97: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.5287 - accuracy: 0.4974 - val_loss: 1.5204 - val_accuracy: 0.4756\n",
      "Epoch 98/100\n",
      "1239/1250 [============================>.] - ETA: 0s - loss: 1.5475 - accuracy: 0.4896\n",
      "Epoch 98: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5472 - accuracy: 0.4897 - val_loss: 1.3755 - val_accuracy: 0.5203\n",
      "Epoch 99/100\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 1.5541 - accuracy: 0.4898\n",
      "Epoch 99: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5540 - accuracy: 0.4897 - val_loss: 1.4758 - val_accuracy: 0.4972\n",
      "Epoch 100/100\n",
      "1242/1250 [============================>.] - ETA: 0s - loss: 1.5522 - accuracy: 0.4872\n",
      "Epoch 100: val_loss did not improve from 0.85190\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5523 - accuracy: 0.4870 - val_loss: 1.2348 - val_accuracy: 0.5740\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.2)\n",
    "\n",
    "\n",
    "from tensorflow import keras \n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "model=Sequential( [\n",
    "    \n",
    "    Conv2D(filters=16,kernel_size=2,padding='same',activation='relu',\n",
    "           input_shape=(32,32,3))\n",
    "    ,MaxPooling2D(pool_size=2)\n",
    "    ,Conv2D(filters=32,kernel_size=2,padding='same',activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(filters=64,kernel_size=2,padding='same',activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(500,activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(units=10,activation='softmax')    \n",
    "    \n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='cifer_10 _weights_RANDOM.hdf5', verbose=1, save_best_only=True)\n",
    "hist=model.fit(X_train,Y_train,epochs=100,batch_size=32,validation_data=(x_test,y_test),callbacks=[checkpointer],verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c623e5f",
   "metadata": {
    "papermill": {
     "duration": 1.769226,
     "end_time": "2023-08-12T16:50:58.829672",
     "exception": false,
     "start_time": "2023-08-12T16:50:57.060446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "SO THE MODEL SUCKS ASS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1360.106761,
   "end_time": "2023-08-12T16:51:04.235981",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-12T16:28:24.129220",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
